{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "First_tutorial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNrlTftIar2bY2qrVKvz6Ds",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/data-prom/Kaggle_daily_commit/blob/master/First_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4fZYcF-xUED",
        "colab_type": "text"
      },
      "source": [
        "dot 과 matmul의 차이\n",
        "  dot: 행렬의 내적곱\n",
        "  matmul : 두 배열의 행렬곱\n",
        "  https://m.blog.naver.com/PostView.nhn?blogId=cjh226&logNo=221356884894&proxyReferer=https:%2F%2Fwww.google.com%2F"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlTKxfZquzz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "\n",
        "# two-layer network\n",
        "# Naming N is Batch_size, D_in is Input_dim\n",
        "# H is Hidden_dim, D_out is Output_dim\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Generate random input, output data\n",
        "# randn : generate matrix(m,n) over Z-dist random number\n",
        "x = np.random.randn(N, D_in)\n",
        "y = np.random.randn(N, D_out)\n",
        "\n",
        "# Random weight reset\n",
        "w1 = np.random.randn(D_in, H)\n",
        "w2 = np.random.randn(H, D_out)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "  # Forward propagation step: calculate predicted value(y)\n",
        "  h = x.dot(w1)\n",
        "  h_relu = np.maximum(h, 0)\n",
        "  y_pred = h_relu.dot(w2)\n",
        "\n",
        "  # Calculate and print loss(Euclidean distance)\n",
        "  loss = np.square(y_pred -y).sum()\n",
        "  print(t, loss)\n",
        "\n",
        "  # Back propagate with calculated between loss and w1,w2\n",
        "  grad_y_pred = 2.0 * (y_pred - y)\n",
        "  grad_w2 = h_relu.T.dot(grad_y_pred)\n",
        "  grad_h_relu = grad_y_pred.dot(w2.T)\n",
        "  grad_h = grad_h_relu.copy()\n",
        "  grad_h[h < 0] = 0\n",
        "  grad_w1 = x.T.dot(grad_h)\n",
        "\n",
        "  # Update weights\n",
        "  w1 -= learning_rate * grad_w1\n",
        "  w2 -= learning_rate * grad_w2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SipHmBP0xTas",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "ec10420d-f021-4a24-d410-b2e0e0ca9f3d"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "# Almost all settings are same\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "x = torch.randn(N, D_in, device = device, dtype = dtype)\n",
        "y = torch.randn(N, D_out, device = device, dtype = dtype)\n",
        "\n",
        "w1 = torch.randn(D_in, H, device = device, dtype = dtype)\n",
        "w2 = torch.randn(H, D_out, device = device, dtype = dtype)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "  h = x.mm(w1)\n",
        "  h_relu = h.clamp(min = 0)\n",
        "  y_pred = h_relu.mm(w2)\n",
        "\n",
        "  loss = (y_pred - y).pow(2).sum().item()\n",
        "  if t % 100 == 99:\n",
        "    print(t, loss)\n",
        "  \n",
        "  grad_y_pred = 2.0 * (y_pred -y)\n",
        "  grad_w2 = h_relu.t().mm(grad_y_pred)\n",
        "  grad_h_relu = grad_y_pred.mm(w2.t())\n",
        "  grad_h = grad_h_relu.clone()\n",
        "  grad_h[h < 0] = 0\n",
        "  grad_w1 = x.t().mm(grad_h)\n",
        "\n",
        "  w1 -= learning_rate * grad_w1\n",
        "  w2 -= learning_rate * grad_w2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 508.1109619140625\n",
            "199 2.250030755996704\n",
            "299 0.016785353422164917\n",
            "399 0.00036631900002248585\n",
            "499 5.1921335398219526e-05\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}